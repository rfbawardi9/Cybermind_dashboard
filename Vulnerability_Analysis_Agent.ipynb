{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1 Importing libraries:**\n"
      ],
      "metadata": {
        "id": "3SITxeN823Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from dateutil import parser as dtp"
      ],
      "metadata": {
        "id": "7OhWG8GU27E7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 : Clean & Normalize Vulnerability Data**"
      ],
      "metadata": {
        "id": "atDN45u83Qrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from dateutil import parser as dtp\n",
        "\n",
        "def parse_date_safe(x):\n",
        "    try:\n",
        "        return pd.to_datetime(x, errors=\"coerce\")\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "SEV_ALIASES = {\n",
        "    \"NONE\": \"NONE\", \"LOW\": \"LOW\", \"MEDIUM\": \"MEDIUM\", \"HIGH\": \"HIGH\", \"CRITICAL\": \"CRITICAL\",\n",
        "    \"MODERATE\": \"MEDIUM\",\n",
        "    \"INFO\": \"NONE\", \"INFORMATIONAL\": \"NONE\",\n",
        "    \"NA\": \"UNKNOWN\", \"N/A\": \"UNKNOWN\", \"UNKNOWN\": \"UNKNOWN\"\n",
        "}\n",
        "\n",
        "def normalize_sev_text(s):\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    t = str(s).strip().upper()\n",
        "    return SEV_ALIASES.get(t, t)\n",
        "\n",
        "def sev_from_cvss(score):\n",
        "    if score is None or pd.isna(score):\n",
        "        return None\n",
        "    try:\n",
        "        score = float(score)\n",
        "    except Exception:\n",
        "        return None\n",
        "    if score == 0.0:\n",
        "        return \"NONE\"\n",
        "    if 0.0 < score <= 3.9:\n",
        "        return \"LOW\"\n",
        "    if 4.0 <= score <= 6.9:\n",
        "        return \"MEDIUM\"\n",
        "    if 7.0 <= score <= 8.9:\n",
        "        return \"HIGH\"\n",
        "    return \"CRITICAL\"\n",
        "\n",
        "# main cleaning function\n",
        "\n",
        "def clean_and_normalize_vulns(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Takes a raw CVE dataframe (from Threat Collector or CSV)\n",
        "    and returns a cleaned + normalized dataframe.\n",
        "    \"\"\"\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # Summary before\n",
        "    print(\"Rows (raw):\", len(df))\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "    # 1) Remove accidental header-duplicate rows like a literal 'id' in id column\n",
        "    if df['id'].astype(str).str.lower().eq('id').any():\n",
        "        n_before = len(df)\n",
        "        df = df[~df['id'].astype(str).str.lower().eq('id')].copy()\n",
        "        print(f\"Removed {n_before - len(df)} stray header rows\")\n",
        "\n",
        "    # 2) Drop exact duplicates\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    print(f\"Dropped exact duplicates: {before - after}\")\n",
        "\n",
        "    # 3) Convert / normalize columns\n",
        "    # cvss_score -> numeric\n",
        "    df['cvss_score'] = pd.to_numeric(\n",
        "        df.get('cvss_score', pd.Series([np.nan] * len(df))),\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    # Keep original severity text for auditing, then normalize to our canonical set\n",
        "    df['original_severity_raw'] = df.get('severity')\n",
        "    df['severity_text_norm'] = df['original_severity_raw'].apply(normalize_sev_text)\n",
        "\n",
        "    # parsed dates\n",
        "    for date_col in ['published_date', 'last_modified', 'ingested_at']:\n",
        "        if date_col in df.columns:\n",
        "            df[date_col] = df[date_col].apply(parse_date_safe)\n",
        "\n",
        "    # 4) Decide final severity: prefer cvss numeric if available\n",
        "    def decide_final_severity(row):\n",
        "        score = row.get('cvss_score')\n",
        "        sev_txt = row.get('severity_text_norm')\n",
        "        sev_from_score = sev_from_cvss(score)\n",
        "        if sev_from_score and not sev_txt:\n",
        "            return sev_from_score, \"score\"\n",
        "        if sev_from_score and sev_txt:\n",
        "            if sev_from_score != sev_txt:\n",
        "                return sev_from_score, \"score_over_text\"\n",
        "            return sev_txt, \"score_and_text_agree\"\n",
        "        if not sev_from_score and sev_txt:\n",
        "            return sev_txt, \"text_only\"\n",
        "        return \"UNKNOWN\", \"none\"\n",
        "\n",
        "    decisions = df.apply(decide_final_severity, axis=1, result_type='expand')\n",
        "    decisions.columns = ['final_severity', 'severity_source']\n",
        "    df[['final_severity', 'severity_source']] = decisions\n",
        "\n",
        "    # Adding a quick helper\n",
        "    df['has_cvss'] = df['cvss_score'].notna()\n",
        "\n",
        "    # 5) Quick checks\n",
        "    print(\"\\nFinal severity distribution:\")\n",
        "    print(df['final_severity'].value_counts(dropna=False))\n",
        "\n",
        "    # IMPORTANT: no saving here, we just return the dataframe\n",
        "    return df"
      ],
      "metadata": {
        "id": "w3-7kKZ93kIQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtaS71r-4XUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Technique Matching (MITRE ATT&CK Enrichment)**"
      ],
      "metadata": {
        "id": "7iMlI6kr4qOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "STOP = set([\n",
        "    \"the\",\"and\",\"with\",\"from\",\"that\",\"this\",\"into\",\"over\",\"under\",\"using\",\"used\",\n",
        "    \"will\",\"have\",\"been\",\"their\",\"they\",\"them\",\"such\",\"also\",\"able\",\"allow\",\"allows\",\n",
        "    \"may\",\"can\",\"for\",\"onto\",\"than\",\"then\",\"when\",\"where\",\"while\",\"within\",\n",
        "    \"task\",\"tasks\"\n",
        "])\n",
        "\n",
        "def build_keywords(text: str):\n",
        "    txt = str(text).lower()\n",
        "    toks = re.findall(r\"[a-z0-9][a-z0-9\\-\\_]{2,}\", txt)  # tokens len >= 3\n",
        "    toks = [t for t in toks if t not in STOP]\n",
        "    return set(toks)\n",
        "\n",
        "def build_tech_index(df_mitre: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Build a keyword index from a MITRE techniques dataframe.\n",
        "    Expects columns like: tech_id, name, description\n",
        "    \"\"\"\n",
        "    df_mitre = df_mitre.fillna(\"\")\n",
        "    tech_index = []\n",
        "    for _, r in df_mitre.iterrows():\n",
        "        name = r.get(\"name\", \"\")\n",
        "        desc = r.get(\"description\", \"\")\n",
        "        kws  = build_keywords(name) | build_keywords(desc)\n",
        "        tech_index.append({\n",
        "            \"tech_id\":   r.get(\"tech_id\"),\n",
        "            \"tech_name\": name,\n",
        "            \"keywords\":  kws\n",
        "        })\n",
        "    print(\"Built keyword index for\", len(tech_index), \"techniques\")\n",
        "    return tech_index\n",
        "\n",
        "def match_technique_for_text(text: str, tech_index, min_score: int = 1):\n",
        "    \"\"\"\n",
        "    Given a free-text description and a pre-built tech_index,\n",
        "    return (tech_id, tech_name, score).\n",
        "    \"\"\"\n",
        "    text_kws = build_keywords(text)\n",
        "    best = None\n",
        "    best_score = 0\n",
        "    for tech in tech_index:\n",
        "        score = len(text_kws & tech[\"keywords\"])\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best = tech\n",
        "    if best and best_score >= min_score:\n",
        "        return best[\"tech_id\"], best[\"tech_name\"], int(best_score)\n",
        "    return None, None, 0\n",
        "\n",
        "#  MITRE ENRICHMENT\n",
        "\n",
        "def enrich_with_mitre(df_cve: pd.DataFrame,\n",
        "                      df_mitre: pd.DataFrame,\n",
        "                      min_score: int = 1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Takes:\n",
        "      - df_cve   : cleaned vulnerabilities (output of Step 2)\n",
        "      - df_mitre : MITRE ATT&CK techniques table\n",
        "    Returns:\n",
        "      - df_enriched : df_cve + ['technique_id','technique_name','tech_match_score']\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"CVE rows:\", len(df_cve))\n",
        "    print(\"MITRE techniques:\", len(df_mitre))\n",
        "\n",
        "    # build index from MITRE table\n",
        "    tech_index = build_tech_index(df_mitre)\n",
        "\n",
        "    # helper to combine title + description safely\n",
        "    def merged_text(row):\n",
        "        return f\"{row.get('title', '')} {row.get('description', '')}\"\n",
        "\n",
        "    # apply matching to every CVE\n",
        "    matches = df_cve.apply(\n",
        "        lambda r: match_technique_for_text(merged_text(r), tech_index, min_score=min_score),\n",
        "        axis=1,\n",
        "        result_type=\"expand\"\n",
        "    )\n",
        "    matches.columns = [\"technique_id\", \"technique_name\", \"tech_match_score\"]\n",
        "\n",
        "    df_enriched = pd.concat([df_cve, matches], axis=1)\n",
        "\n",
        "    # coverage / sanity check\n",
        "    matched = (df_enriched[\"technique_id\"].notna()) & (df_enriched[\"tech_match_score\"] > 0)\n",
        "    print(f\"\\nMatched rows: {matched.sum()} / {len(df_enriched)} \"\n",
        "          f\"({matched.sum()/len(df_enriched):.1%})\")\n",
        "\n",
        "    print(\"\\nTop 5 enriched rows (preview):\")\n",
        "    preview_cols = [\n",
        "        \"id\",\n",
        "        \"title\",\n",
        "        \"final_severity\" if \"final_severity\" in df_enriched.columns else \"severity\",\n",
        "        \"technique_name\",\n",
        "        \"technique_id\",\n",
        "        \"tech_match_score\"\n",
        "    ]\n",
        "    display(df_enriched.head(5)[preview_cols])\n",
        "\n",
        "    # IMPORTANT: no CSV saving here â€” just return the enriched dataframe\n",
        "    return df_enriched"
      ],
      "metadata": {
        "id": "FV4BxMkm6zVT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Risk Scoring, Prioritization & Remediation Suggestions**"
      ],
      "metadata": {
        "id": "JC5RgkKy7NeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def risk_scoring_and_prioritization(df_enriched: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Step 4: Risk Scoring, Prioritization & Remediation Suggestions\n",
        "\n",
        "    Input:\n",
        "        df_enriched - DataFrame from Step 3 with columns like:\n",
        "            ['id','title','cvss_score','final_severity',\n",
        "             'technique_name','technique_id','tech_match_score','affected_products',...]\n",
        "    Output:\n",
        "        df_prior   - prioritized vulnerabilities with risk_score, priority, remediation_suggestion\n",
        "        df_rem     - compact remediation view\n",
        "        prod_summary - per-product summary table\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_enriched.copy()\n",
        "\n",
        "    # 2) Prepare numeric fields and handle missing values\n",
        "    df['cvss_score'] = pd.to_numeric(df.get('cvss_score'), errors='coerce').fillna(0.0)\n",
        "    df['tech_match_score'] = pd.to_numeric(df.get('tech_match_score'), errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Compute maximum observed tech match score (used for normalization)\n",
        "    max_tech_score = max(1, int(df['tech_match_score'].max()))\n",
        "    print(\"Max tech_match_score observed:\", max_tech_score)\n",
        "\n",
        "    # 3) Risk score formula (simple, tunable)\n",
        "    # - CVSS (0-10) contributes 70% of the score\n",
        "    # - MITRE tech match normalized contributes 30%\n",
        "    def compute_risk_score(row):\n",
        "        cvss = float(row['cvss_score']) if not np.isnan(row['cvss_score']) else 0.0\n",
        "        tv = float(row['tech_match_score']) / max_tech_score  # 0..1\n",
        "        # scale to 0..100\n",
        "        score = (cvss / 10.0) * 70.0 + tv * 30.0\n",
        "        return round(score, 1)\n",
        "\n",
        "    df['risk_score'] = df.apply(compute_risk_score, axis=1)\n",
        "\n",
        "    # 4) Priority rules (combine final_severity and risk_score)\n",
        "    def assign_priority(row):\n",
        "        sev = str(row.get('final_severity') or row.get('severity') or \"\").upper()\n",
        "        s = float(row['risk_score'])\n",
        "        if sev == 'CRITICAL' or s >= 80:\n",
        "            return \"CRITICAL\"\n",
        "        if sev == 'HIGH' or s >= 60:\n",
        "            return \"HIGH\"\n",
        "        if sev == 'MEDIUM' or s >= 40:\n",
        "            return \"MEDIUM\"\n",
        "        if s >= 20:\n",
        "            return \"LOW\"\n",
        "        return \"INFO\"\n",
        "\n",
        "    df['priority'] = df.apply(assign_priority, axis=1)\n",
        "\n",
        "    # 5) Simple remediation suggestion rules\n",
        "    def suggest_remediation(row):\n",
        "        tech = str(row.get('technique_name') or \"\").lower()\n",
        "        title = str(row.get('title') or \"\").lower()\n",
        "        cwe = str(row.get('cwe') or \"\").lower()\n",
        "\n",
        "        if \"registry\" in tech or \"registry\" in title:\n",
        "            return \"Apply vendor patch / registry hardening / restrict write access\"\n",
        "        if \"dll\" in tech or \"dll\" in title:\n",
        "            return \"Update/replace vulnerable DLLs, validate signatures, apply vendor patch\"\n",
        "        if \"remote code\" in title or \"remote\" in tech or \"rce\" in title:\n",
        "            return \"Apply vendor patch immediately, restrict network access, enable WAF/IDS\"\n",
        "        if \"cloud\" in tech or \"iam\" in tech or \"cloud\" in title:\n",
        "            return \"Review cloud IAM policies, rotate credentials, apply vendor patch\"\n",
        "        if \"sql\" in title or \"sql injection\" in title or \"injection\" in tech:\n",
        "            return \"Sanitize inputs, apply patch, review DB permissions and WAF rules\"\n",
        "        if \"modify registry\" in tech:\n",
        "            return \"Block unauthorized registry changes and apply vendor fixes\"\n",
        "        # fallback\n",
        "        return \"Check vendor advisory for patch; apply patch and follow recommended mitigations\"\n",
        "\n",
        "    df['remediation_suggestion'] = df.apply(suggest_remediation, axis=1)\n",
        "\n",
        "    # 6) Per-product summary\n",
        "    df['affected_products'] = df.get('affected_products', pd.Series([None]*len(df))).fillna(\"UNKNOWN_PRODUCT\")\n",
        "\n",
        "    prod_summary = (\n",
        "        df.groupby(['affected_products','priority'])\n",
        "          .size()\n",
        "          .reset_index(name='count')\n",
        "          .pivot(index='affected_products', columns='priority', values='count')\n",
        "          .fillna(0)\n",
        "          .reset_index()\n",
        "    )\n",
        "\n",
        "    # main prioritized view (same columns as before)\n",
        "    cols_out = [\n",
        "        'id','title','cvss_score','final_severity','technique_name','technique_id',\n",
        "        'tech_match_score','risk_score','priority','remediation_suggestion','affected_products'\n",
        "    ]\n",
        "    df_prior = df[cols_out].copy()\n",
        "\n",
        "    # compact remediation view\n",
        "    df_rem = df[['id','remediation_suggestion','priority','risk_score']].copy()\n",
        "\n",
        "    print(\"\\nTop 8 prioritized vulnerabilities (preview):\")\n",
        "    display(df_prior.sort_values(['priority','risk_score'], ascending=[True, False]).head(8))\n",
        "\n",
        "    # return 3 tables instead of saving CSV files\n",
        "    return df_prior, df_rem, prod_summary\n"
      ],
      "metadata": {
        "id": "2FVsEFGI9xlE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Build the Vulnerability Analyzer Agent**"
      ],
      "metadata": {
        "id": "Trkkeafn-TKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "class VulnerabilityAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzer built on top of the prioritized vulnerabilities produced in Step 4.\n",
        "    It does NOT read or write CSV files.\n",
        "    It only uses in-memory DataFrames provided by the pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df_prior: pd.DataFrame, df_rem: Optional[pd.DataFrame] = None):\n",
        "        # Work on copies to avoid changing original DataFrames\n",
        "        self.df_prior = df_prior.copy()\n",
        "        self.df_rem = df_rem.copy() if df_rem is not None else None\n",
        "\n",
        "        # Normalize helpful columns\n",
        "        self.df_prior['risk_score'] = pd.to_numeric(\n",
        "            self.df_prior.get('risk_score', 0),\n",
        "            errors='coerce'\n",
        "        ).fillna(0)\n",
        "\n",
        "        self.df_prior['tech_match_score'] = pd.to_numeric(\n",
        "            self.df_prior.get('tech_match_score', 0),\n",
        "            errors='coerce'\n",
        "        ).fillna(0)\n",
        "\n",
        "        self.df_prior['cvss_score'] = pd.to_numeric(\n",
        "            self.df_prior.get('cvss_score', 0),\n",
        "            errors='coerce'\n",
        "        ).fillna(0)\n",
        "\n",
        "        if 'id' in self.df_prior.columns:\n",
        "            self.df_prior['id'] = self.df_prior['id'].astype(str).str.strip()\n",
        "\n",
        "        print(\"VulnerabilityAnalyzer initialized with\", len(self.df_prior), \"rows\")\n",
        "\n",
        "    # 1) Find by CVE id\n",
        "    def find_by_cve(self, cve_id: str) -> Optional[dict]:\n",
        "        \"\"\"Return a single vulnerability row as dict for the given CVE id (case-insensitive).\"\"\"\n",
        "        cve_id = str(cve_id).strip()\n",
        "        if 'id' not in self.df_prior.columns:\n",
        "            return None\n",
        "        row = self.df_prior[self.df_prior['id'].str.lower() == cve_id.lower()]\n",
        "        if row.empty:\n",
        "            return None\n",
        "        return row.iloc[0].to_dict()\n",
        "\n",
        "    # 2) Top N by risk\n",
        "    def top_n_by_risk(self, n: int = 10, severity: Optional[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return top-N rows by risk_score. Optionally filter by final_severity.\"\"\"\n",
        "        df = self.df_prior.copy()\n",
        "        if severity and 'final_severity' in df.columns:\n",
        "            df = df[df['final_severity'].str.upper() == severity.upper()]\n",
        "        return df.sort_values('risk_score', ascending=False).head(n)\n",
        "\n",
        "    # 3) Free-text search\n",
        "    def search_free_text(self, query: str, n: int = 20) -> pd.DataFrame:\n",
        "        \"\"\"Simple text search over title + description + technique_name + affected_products.\"\"\"\n",
        "        q = str(query).lower().strip()\n",
        "        cols = ['title', 'description', 'technique_name', 'affected_products']\n",
        "        mask = False\n",
        "        for c in cols:\n",
        "            if c in self.df_prior.columns:\n",
        "                mask = mask | self.df_prior[c].astype(str).str.lower().str.contains(q, na=False)\n",
        "        if isinstance(mask, bool) and mask is False:\n",
        "            return pd.DataFrame()\n",
        "        results = self.df_prior[mask].copy()\n",
        "        # sort by risk and tech_match_score so more relevant appear first\n",
        "        results['sort_key'] = results['risk_score'] * 2 + results['tech_match_score']\n",
        "        return results.sort_values('sort_key', ascending=False).head(n).drop(columns=['sort_key'])\n",
        "\n",
        "    # 4) Filter by product name\n",
        "    def by_product(self, product_name: str, n: int = 50) -> pd.DataFrame:\n",
        "        \"\"\"Return vulnerabilities mentioning product_name in affected_products.\"\"\"\n",
        "        p = str(product_name).lower()\n",
        "        if 'affected_products' not in self.df_prior.columns:\n",
        "            return pd.DataFrame()\n",
        "        df = self.df_prior[\n",
        "            self.df_prior['affected_products'].astype(str).str.lower().str.contains(p, na=False)\n",
        "        ].copy()\n",
        "        return df.sort_values('risk_score', ascending=False).head(n)\n",
        "\n",
        "    # 5) Get remediation suggestion for a CVE\n",
        "    def remediation_for_cve(self, cve_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Return remediation_suggestion from prioritized table\n",
        "        or from a separate remediation dataframe if available.\n",
        "        \"\"\"\n",
        "        row = self.find_by_cve(cve_id)\n",
        "        if row is None:\n",
        "            return None\n",
        "\n",
        "        # try field on prioritized table first\n",
        "        if 'remediation_suggestion' in row and pd.notna(row['remediation_suggestion']):\n",
        "            return row['remediation_suggestion']\n",
        "\n",
        "        # fallback to separate remediation file\n",
        "        if (self.df_rem is not None and\n",
        "            'id' in self.df_rem.columns and\n",
        "            'remediation_suggestion' in self.df_rem.columns):\n",
        "            r = self.df_rem[self.df_rem['id'].astype(str).str.lower() == str(cve_id).lower()]\n",
        "            if not r.empty:\n",
        "                return r.iloc[0]['remediation_suggestion']\n",
        "\n",
        "        return None\n",
        "\n",
        "    # 6) Export a single CVE record as JSON (string)\n",
        "    def export_vuln_report_json(self, cve_id: str) -> str:\n",
        "        \"\"\"\n",
        "        Export a single CVE report as a JSON string.\n",
        "        (In an API, you might return this directly to the caller.)\n",
        "        \"\"\"\n",
        "        rec = self.find_by_cve(cve_id)\n",
        "        if rec is None:\n",
        "            raise ValueError(\"CVE not found: \" + str(cve_id))\n",
        "        rec_serializable = json.loads(json.dumps(rec, default=str))\n",
        "        return json.dumps(rec_serializable, indent=2, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "bt_LcGQg-iIK"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}